{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "list_ = os.listdir('sorted_data/128/Trajectory/')\n",
    "sorted_41 = []\n",
    "for i in range(len(list_)):\n",
    "    file_ = open('sorted_data/128/Trajectory/' + list_[i], 'r') \n",
    "    file = file_.readlines()\n",
    "    file = file[6:]\n",
    "    for j in range(len(file)):\n",
    "        tmp = file[j].split(',')\n",
    "        sorted_41.append([tmp[0], tmp[1], tmp[-2] + ' ' +tmp[-1][:-1]])\n",
    "\n",
    "df = pd.DataFrame(sorted_41)\n",
    "df[2] = pd.to_datetime(df[2])\n",
    "df.sort_values(by = [2],inplace=True)\n",
    "df = df.reset_index()\n",
    "df = df[[0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    " \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "in_seq1 = array([x for x in range(0,100,10)])\n",
    "in_seq2 = array([x for x in range(5,105,10)])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5],\n",
       "       [ 25],\n",
       "       [ 45],\n",
       "       [ 65],\n",
       "       [ 85],\n",
       "       [105],\n",
       "       [125],\n",
       "       [145],\n",
       "       [165],\n",
       "       [185]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0                  40.07585\n",
       " 1          40.0758333333333\n",
       " 2                    40.076\n",
       " 3                   40.0763\n",
       " 4                   40.0766\n",
       "                  ...       \n",
       " 1208495          39.9756416\n",
       " 1208496            39.97564\n",
       " 1208497          39.9756233\n",
       " 1208498          39.9755883\n",
       " 1208499            39.97557\n",
       " Name: 0, Length: 1208500, dtype: object,\n",
       " 0          116.327816666667\n",
       " 1          116.327583333333\n",
       " 2          116.327566666667\n",
       " 3                  116.3275\n",
       " 4          116.327416666667\n",
       "                  ...       \n",
       " 1208495         116.3302849\n",
       " 1208496         116.3302566\n",
       " 1208497         116.3302866\n",
       " 1208498         116.3303116\n",
       " 1208499           116.33036\n",
       " Name: 1, Length: 1208500, dtype: object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0], df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MV_LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features,seq_length):\n",
    "        super(MV_LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = 20 # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "    \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers, \n",
    "                                 batch_first = True)\n",
    "        # according to pytorch docs LSTM output is \n",
    "        # (batch_size,seq_len, num_directions * hidden_size)\n",
    "        # when considering batch_first = True\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, 1)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n",
    "        # lstm_out(with batch_first = True) is \n",
    "        # (batch_size,seq_len,num_directions * hidden_size)\n",
    "        # for following linear layer we want to keep batch_size dimension and merge rest       \n",
    "        # .contiguous() -> solves tensor compatibility error\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        return self.l_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3, 2) (2, 8)\n"
     ]
    }
   ],
   "source": [
    "n_features = 1 # this is number of parallel inputs\n",
    "n_timesteps = 3 # this is number of timesteps\n",
    "\n",
    "# convert dataset into input/output\n",
    "X, y = split_sequences(dataset, n_timesteps)\n",
    "y = np.vstack([y,y])\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# create NN\n",
    "mv_net_x = MV_LSTM(n_features,n_timesteps)\n",
    "mv_net_y = MV_LSTM(n_features, n_timesteps)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer_x = torch.optim.Adam(mv_net_x.parameters(), lr=1e-1)\n",
    "optimizer_y = torch.optim.Adam(mv_net_y.parameters(), lr=1e-1)\n",
    "\n",
    "train_episodes = 500\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  5],\n",
       "        [10, 15],\n",
       "        [20, 25]],\n",
       "\n",
       "       [[10, 15],\n",
       "        [20, 25],\n",
       "        [30, 35]],\n",
       "\n",
       "       [[20, 25],\n",
       "        [30, 35],\n",
       "        [40, 45]],\n",
       "\n",
       "       [[30, 35],\n",
       "        [40, 45],\n",
       "        [50, 55]],\n",
       "\n",
       "       [[40, 45],\n",
       "        [50, 55],\n",
       "        [60, 65]],\n",
       "\n",
       "       [[50, 55],\n",
       "        [60, 65],\n",
       "        [70, 75]],\n",
       "\n",
       "       [[60, 65],\n",
       "        [70, 75],\n",
       "        [80, 85]],\n",
       "\n",
       "       [[70, 75],\n",
       "        [80, 85],\n",
       "        [90, 95]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0 loss :  30562.6953125\n",
      "step :  1 loss :  29500.064453125\n",
      "step :  2 loss :  27658.447265625\n",
      "step :  3 loss :  26146.0703125\n",
      "step :  4 loss :  24701.228515625\n",
      "step :  5 loss :  23122.08203125\n",
      "step :  6 loss :  21718.173828125\n",
      "step :  7 loss :  20306.89453125\n",
      "step :  8 loss :  18937.60546875\n",
      "step :  9 loss :  17651.4921875\n",
      "step :  10 loss :  16332.0771484375\n",
      "step :  11 loss :  15099.9267578125\n",
      "step :  12 loss :  13888.5546875\n",
      "step :  13 loss :  12799.01171875\n",
      "step :  14 loss :  11760.1220703125\n",
      "step :  15 loss :  10694.171875\n",
      "step :  16 loss :  9778.779296875\n",
      "step :  17 loss :  8992.82421875\n",
      "step :  18 loss :  8275.5146484375\n",
      "step :  19 loss :  7623.88916015625\n",
      "step :  20 loss :  7036.7177734375\n",
      "step :  21 loss :  6511.962890625\n",
      "step :  22 loss :  6046.8857421875\n",
      "step :  23 loss :  5638.181640625\n",
      "step :  24 loss :  5282.0966796875\n",
      "step :  25 loss :  4974.5810546875\n",
      "step :  26 loss :  4711.4248046875\n",
      "step :  27 loss :  4488.4072265625\n",
      "step :  28 loss :  4301.4375\n",
      "step :  29 loss :  4146.6669921875\n",
      "step :  30 loss :  4020.577392578125\n",
      "step :  31 loss :  3920.00537109375\n",
      "step :  32 loss :  3842.08984375\n",
      "step :  33 loss :  3784.11865234375\n",
      "step :  34 loss :  3743.350341796875\n",
      "step :  35 loss :  3716.877197265625\n",
      "step :  36 loss :  3701.42724609375\n",
      "step :  37 loss :  3692.4775390625\n",
      "step :  38 loss :  3683.485107421875\n",
      "step :  39 loss :  3729.51953125\n",
      "step :  40 loss :  3692.78369140625\n",
      "step :  41 loss :  3709.69091796875\n",
      "step :  42 loss :  3719.24755859375\n",
      "step :  43 loss :  3718.33203125\n",
      "step :  44 loss :  3705.61865234375\n",
      "step :  45 loss :  3678.7724609375\n",
      "step :  46 loss :  3686.649658203125\n",
      "step :  47 loss :  3651.885009765625\n",
      "step :  48 loss :  3563.05224609375\n",
      "step :  49 loss :  3442.173095703125\n",
      "step :  50 loss :  3223.939453125\n",
      "step :  51 loss :  2936.07763671875\n",
      "step :  52 loss :  2954.87939453125\n",
      "step :  53 loss :  2811.865478515625\n",
      "step :  54 loss :  2952.767822265625\n",
      "step :  55 loss :  2954.6767578125\n",
      "step :  56 loss :  2612.67919921875\n",
      "step :  57 loss :  2558.2041015625\n",
      "step :  58 loss :  2583.47998046875\n",
      "step :  59 loss :  2691.39111328125\n",
      "step :  60 loss :  2681.421630859375\n",
      "step :  61 loss :  2460.28076171875\n",
      "step :  62 loss :  2320.026123046875\n",
      "step :  63 loss :  2358.292236328125\n",
      "step :  64 loss :  2098.01904296875\n",
      "step :  65 loss :  2155.759765625\n",
      "step :  66 loss :  2115.7978515625\n",
      "step :  67 loss :  1986.516845703125\n",
      "step :  68 loss :  1820.220458984375\n",
      "step :  69 loss :  1737.123779296875\n",
      "step :  70 loss :  1685.947998046875\n",
      "step :  71 loss :  1654.3916015625\n",
      "step :  72 loss :  1570.35205078125\n",
      "step :  73 loss :  1483.09619140625\n",
      "step :  74 loss :  1445.2626953125\n",
      "step :  75 loss :  1313.011962890625\n",
      "step :  76 loss :  1237.88232421875\n",
      "step :  77 loss :  1186.7694091796875\n",
      "step :  78 loss :  1135.025634765625\n",
      "step :  79 loss :  1062.197265625\n",
      "step :  80 loss :  979.5842895507812\n",
      "step :  81 loss :  959.27685546875\n",
      "step :  82 loss :  834.2249755859375\n",
      "step :  83 loss :  776.14697265625\n",
      "step :  84 loss :  806.9642944335938\n",
      "step :  85 loss :  819.721923828125\n",
      "step :  86 loss :  699.1209716796875\n",
      "step :  87 loss :  657.904296875\n",
      "step :  88 loss :  623.1053466796875\n",
      "step :  89 loss :  582.62255859375\n",
      "step :  90 loss :  535.342529296875\n",
      "step :  91 loss :  480.494873046875\n",
      "step :  92 loss :  448.69390869140625\n",
      "step :  93 loss :  424.0500793457031\n",
      "step :  94 loss :  390.43426513671875\n",
      "step :  95 loss :  363.3773193359375\n",
      "step :  96 loss :  342.5030212402344\n",
      "step :  97 loss :  316.83966064453125\n",
      "step :  98 loss :  293.839111328125\n",
      "step :  99 loss :  271.9381103515625\n",
      "step :  100 loss :  250.62603759765625\n",
      "step :  101 loss :  234.70217895507812\n",
      "step :  102 loss :  217.0748291015625\n",
      "step :  103 loss :  203.7900848388672\n",
      "step :  104 loss :  190.42633056640625\n",
      "step :  105 loss :  176.21873474121094\n",
      "step :  106 loss :  165.95718383789062\n",
      "step :  107 loss :  152.7281036376953\n",
      "step :  108 loss :  142.82843017578125\n",
      "step :  109 loss :  134.44447326660156\n",
      "step :  110 loss :  124.33634948730469\n",
      "step :  111 loss :  117.64149475097656\n",
      "step :  112 loss :  110.40756225585938\n",
      "step :  113 loss :  103.44664764404297\n",
      "step :  114 loss :  98.3199691772461\n",
      "step :  115 loss :  91.83089447021484\n",
      "step :  116 loss :  86.57601165771484\n",
      "step :  117 loss :  82.06194305419922\n",
      "step :  118 loss :  77.17777252197266\n",
      "step :  119 loss :  73.87458801269531\n",
      "step :  120 loss :  69.75192260742188\n",
      "step :  121 loss :  66.49317932128906\n",
      "step :  122 loss :  62.84808349609375\n",
      "step :  123 loss :  59.37511444091797\n",
      "step :  124 loss :  56.06415557861328\n",
      "step :  125 loss :  52.430213928222656\n",
      "step :  126 loss :  49.532554626464844\n",
      "step :  127 loss :  46.70948028564453\n",
      "step :  128 loss :  45.765357971191406\n",
      "step :  129 loss :  43.005149841308594\n",
      "step :  130 loss :  40.603736877441406\n",
      "step :  131 loss :  39.03104019165039\n",
      "step :  132 loss :  38.051841735839844\n",
      "step :  133 loss :  35.55049514770508\n",
      "step :  134 loss :  34.06110763549805\n",
      "step :  135 loss :  32.960208892822266\n",
      "step :  136 loss :  31.342910766601562\n",
      "step :  137 loss :  29.772991180419922\n",
      "step :  138 loss :  28.92908477783203\n",
      "step :  139 loss :  27.879051208496094\n",
      "step :  140 loss :  26.580936431884766\n",
      "step :  141 loss :  25.41015625\n",
      "step :  142 loss :  24.557743072509766\n",
      "step :  143 loss :  23.670303344726562\n",
      "step :  144 loss :  22.694751739501953\n",
      "step :  145 loss :  21.77108383178711\n",
      "step :  146 loss :  20.93954086303711\n",
      "step :  147 loss :  20.22612190246582\n",
      "step :  148 loss :  19.330181121826172\n",
      "step :  149 loss :  18.725990295410156\n",
      "step :  150 loss :  18.082935333251953\n",
      "step :  151 loss :  17.32859230041504\n",
      "step :  152 loss :  16.692731857299805\n",
      "step :  153 loss :  16.08932113647461\n",
      "step :  154 loss :  15.510976791381836\n",
      "step :  155 loss :  14.85072135925293\n",
      "step :  156 loss :  14.363039016723633\n",
      "step :  157 loss :  13.842235565185547\n",
      "step :  158 loss :  13.308740615844727\n",
      "step :  159 loss :  12.836467742919922\n",
      "step :  160 loss :  12.355683326721191\n",
      "step :  161 loss :  11.848095893859863\n",
      "step :  162 loss :  11.382593154907227\n",
      "step :  163 loss :  10.992095947265625\n",
      "step :  164 loss :  10.53646469116211\n",
      "step :  165 loss :  10.1463041305542\n",
      "step :  166 loss :  9.790773391723633\n",
      "step :  167 loss :  9.42637825012207\n",
      "step :  168 loss :  9.17456340789795\n",
      "step :  169 loss :  8.970465660095215\n",
      "step :  170 loss :  8.96127700805664\n",
      "step :  171 loss :  8.982682228088379\n",
      "step :  172 loss :  8.943794250488281\n",
      "step :  173 loss :  8.192087173461914\n",
      "step :  174 loss :  7.25826358795166\n",
      "step :  175 loss :  6.90509033203125\n",
      "step :  176 loss :  7.141757011413574\n",
      "step :  177 loss :  8.385086059570312\n",
      "step :  178 loss :  8.344714164733887\n",
      "step :  179 loss :  7.324718475341797\n",
      "step :  180 loss :  5.831547737121582\n",
      "step :  181 loss :  5.992459297180176\n",
      "step :  182 loss :  6.96700382232666\n",
      "step :  183 loss :  6.846034526824951\n",
      "step :  184 loss :  5.4961676597595215\n",
      "step :  185 loss :  4.738474369049072\n",
      "step :  186 loss :  6.762195587158203\n",
      "step :  187 loss :  11.331693649291992\n",
      "step :  188 loss :  9.046067237854004\n",
      "step :  189 loss :  5.138211727142334\n",
      "step :  190 loss :  11.025124549865723\n",
      "step :  191 loss :  12.755154609680176\n",
      "step :  192 loss :  6.289004325866699\n",
      "step :  193 loss :  7.5313720703125\n",
      "step :  194 loss :  6.199857711791992\n",
      "step :  195 loss :  3.8571877479553223\n",
      "step :  196 loss :  6.777967929840088\n",
      "step :  197 loss :  7.572465896606445\n",
      "step :  198 loss :  4.057303428649902\n",
      "step :  199 loss :  8.15381908416748\n",
      "step :  200 loss :  5.507573127746582\n",
      "step :  201 loss :  3.8524527549743652\n",
      "step :  202 loss :  7.887804985046387\n",
      "step :  203 loss :  4.3187031745910645\n",
      "step :  204 loss :  4.75192928314209\n",
      "step :  205 loss :  5.136775493621826\n",
      "step :  206 loss :  2.5556538105010986\n",
      "step :  207 loss :  4.152618408203125\n",
      "step :  208 loss :  4.295158386230469\n",
      "step :  209 loss :  2.5870847702026367\n",
      "step :  210 loss :  3.253386974334717\n",
      "step :  211 loss :  2.5369820594787598\n",
      "step :  212 loss :  2.3280320167541504\n",
      "step :  213 loss :  2.579928159713745\n",
      "step :  214 loss :  2.5341830253601074\n",
      "step :  215 loss :  2.281811237335205\n",
      "step :  216 loss :  2.1821720600128174\n",
      "step :  217 loss :  1.6364400386810303\n",
      "step :  218 loss :  1.913580060005188\n",
      "step :  219 loss :  1.8027310371398926\n",
      "step :  220 loss :  1.3267699480056763\n",
      "step :  221 loss :  1.7734432220458984\n",
      "step :  222 loss :  1.2159533500671387\n",
      "step :  223 loss :  1.322744369506836\n",
      "step :  224 loss :  1.4114594459533691\n",
      "step :  225 loss :  0.9697546362876892\n",
      "step :  226 loss :  1.0062272548675537\n",
      "step :  227 loss :  1.1795477867126465\n",
      "step :  228 loss :  0.8572119474411011\n",
      "step :  229 loss :  0.9655285477638245\n",
      "step :  230 loss :  1.4008488655090332\n",
      "step :  231 loss :  1.0924571752548218\n",
      "step :  232 loss :  0.9344439506530762\n",
      "step :  233 loss :  1.4957141876220703\n",
      "step :  234 loss :  2.506209135055542\n",
      "step :  235 loss :  4.364480972290039\n",
      "step :  236 loss :  4.603600978851318\n",
      "step :  237 loss :  3.469661235809326\n",
      "step :  238 loss :  0.8422958254814148\n",
      "step :  239 loss :  2.461235523223877\n",
      "step :  240 loss :  4.467170715332031\n",
      "step :  241 loss :  1.0263895988464355\n",
      "step :  242 loss :  1.9490653276443481\n",
      "step :  243 loss :  4.8530073165893555\n",
      "step :  244 loss :  1.7168776988983154\n",
      "step :  245 loss :  0.8200274705886841\n",
      "step :  246 loss :  2.856867790222168\n",
      "step :  247 loss :  2.092738151550293\n",
      "step :  248 loss :  0.5574072599411011\n",
      "step :  249 loss :  0.910814642906189\n",
      "step :  250 loss :  1.7554184198379517\n",
      "step :  251 loss :  1.2752143144607544\n",
      "step :  252 loss :  0.3764318823814392\n",
      "step :  253 loss :  0.7558324933052063\n",
      "step :  254 loss :  1.6067979335784912\n",
      "step :  255 loss :  1.5351310968399048\n",
      "step :  256 loss :  0.7427142858505249\n",
      "step :  257 loss :  0.22814223170280457\n",
      "step :  258 loss :  0.38941916823387146\n",
      "step :  259 loss :  0.8647468686103821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  260 loss :  1.105055809020996\n",
      "step :  261 loss :  0.7696412801742554\n",
      "step :  262 loss :  0.3095557689666748\n",
      "step :  263 loss :  0.150666281580925\n",
      "step :  264 loss :  0.36970430612564087\n",
      "step :  265 loss :  0.615480899810791\n",
      "step :  266 loss :  0.5441474318504333\n",
      "step :  267 loss :  0.2917678952217102\n",
      "step :  268 loss :  0.11669367551803589\n",
      "step :  269 loss :  0.171170175075531\n",
      "step :  270 loss :  0.33931970596313477\n",
      "step :  271 loss :  0.4009891450405121\n",
      "step :  272 loss :  0.3254457712173462\n",
      "step :  273 loss :  0.16989833116531372\n",
      "step :  274 loss :  0.08079826086759567\n",
      "step :  275 loss :  0.10187168419361115\n",
      "step :  276 loss :  0.18404985964298248\n",
      "step :  277 loss :  0.24741655588150024\n",
      "step :  278 loss :  0.22404217720031738\n",
      "step :  279 loss :  0.15563467144966125\n",
      "step :  280 loss :  0.07973581552505493\n",
      "step :  281 loss :  0.05260876193642616\n",
      "step :  282 loss :  0.07962819933891296\n",
      "step :  283 loss :  0.12275317311286926\n",
      "step :  284 loss :  0.14706040918827057\n",
      "step :  285 loss :  0.1279773712158203\n",
      "step :  286 loss :  0.08874398469924927\n",
      "step :  287 loss :  0.04912471026182175\n",
      "step :  288 loss :  0.03589083254337311\n",
      "step :  289 loss :  0.04879891127347946\n",
      "step :  290 loss :  0.0712793692946434\n",
      "step :  291 loss :  0.08913307636976242\n",
      "step :  292 loss :  0.08732352405786514\n",
      "step :  293 loss :  0.07246072590351105\n",
      "step :  294 loss :  0.04847968742251396\n",
      "step :  295 loss :  0.030237706378102303\n",
      "step :  296 loss :  0.022962700575590134\n",
      "step :  297 loss :  0.026050925254821777\n",
      "step :  298 loss :  0.03494441136717796\n",
      "step :  299 loss :  0.04402558133006096\n",
      "step :  300 loss :  0.049862559884786606\n",
      "step :  301 loss :  0.047890134155750275\n",
      "step :  302 loss :  0.04238412156701088\n",
      "step :  303 loss :  0.03364814817905426\n",
      "step :  304 loss :  0.025314556434750557\n",
      "step :  305 loss :  0.018290093168616295\n",
      "step :  306 loss :  0.014046309515833855\n",
      "step :  307 loss :  0.01237549353390932\n",
      "step :  308 loss :  0.012749675661325455\n",
      "step :  309 loss :  0.014648817479610443\n",
      "step :  310 loss :  0.017194464802742004\n",
      "step :  311 loss :  0.019972575828433037\n",
      "step :  312 loss :  0.022431181743741035\n",
      "step :  313 loss :  0.02509765513241291\n",
      "step :  314 loss :  0.026999283581972122\n",
      "step :  315 loss :  0.029603969305753708\n",
      "step :  316 loss :  0.031904611736536026\n",
      "step :  317 loss :  0.03609033674001694\n",
      "step :  318 loss :  0.04043370857834816\n",
      "step :  319 loss :  0.04843064770102501\n",
      "step :  320 loss :  0.056882310658693314\n",
      "step :  321 loss :  0.07186917215585709\n",
      "step :  322 loss :  0.08754900842905045\n",
      "step :  323 loss :  0.11605899780988693\n",
      "step :  324 loss :  0.14398567378520966\n",
      "step :  325 loss :  0.19435074925422668\n",
      "step :  326 loss :  0.23269256949424744\n",
      "step :  327 loss :  0.2981937527656555\n",
      "step :  328 loss :  0.31847286224365234\n",
      "step :  329 loss :  0.3519991338253021\n",
      "step :  330 loss :  0.3112166225910187\n",
      "step :  331 loss :  0.2719033360481262\n",
      "step :  332 loss :  0.20015789568424225\n",
      "step :  333 loss :  0.18016013503074646\n",
      "step :  334 loss :  0.1653827577829361\n",
      "step :  335 loss :  0.20071667432785034\n",
      "step :  336 loss :  0.16735674440860748\n",
      "step :  337 loss :  0.13396522402763367\n",
      "step :  338 loss :  0.1429188847541809\n",
      "step :  339 loss :  0.1831727921962738\n",
      "step :  340 loss :  0.20088911056518555\n",
      "step :  341 loss :  0.13949134945869446\n",
      "step :  342 loss :  0.09520518034696579\n",
      "step :  343 loss :  0.10035298764705658\n",
      "step :  344 loss :  0.10779915004968643\n",
      "step :  345 loss :  0.07682600617408752\n",
      "step :  346 loss :  0.05391596257686615\n",
      "step :  347 loss :  0.06553786247968674\n",
      "step :  348 loss :  0.08073561638593674\n",
      "step :  349 loss :  0.06983482092618942\n",
      "step :  350 loss :  0.06768189370632172\n",
      "step :  351 loss :  0.08719999343156815\n",
      "step :  352 loss :  0.11699027568101883\n",
      "step :  353 loss :  0.1292317509651184\n",
      "step :  354 loss :  0.14519861340522766\n",
      "step :  355 loss :  0.16291600465774536\n",
      "step :  356 loss :  0.19737300276756287\n",
      "step :  357 loss :  0.20293593406677246\n",
      "step :  358 loss :  0.19697268307209015\n",
      "step :  359 loss :  0.1570180505514145\n",
      "step :  360 loss :  0.11706256121397018\n",
      "step :  361 loss :  0.07426400482654572\n",
      "step :  362 loss :  0.041674237698316574\n",
      "step :  363 loss :  0.019750341773033142\n",
      "step :  364 loss :  0.009603593498468399\n",
      "step :  365 loss :  0.012264041230082512\n",
      "step :  366 loss :  0.026006372645497322\n",
      "step :  367 loss :  0.04890388622879982\n",
      "step :  368 loss :  0.07564853131771088\n",
      "step :  369 loss :  0.11022032797336578\n",
      "step :  370 loss :  0.14565983414649963\n",
      "step :  371 loss :  0.2014155238866806\n",
      "step :  372 loss :  0.25674575567245483\n",
      "step :  373 loss :  0.3489862084388733\n",
      "step :  374 loss :  0.406721830368042\n",
      "step :  375 loss :  0.46923744678497314\n",
      "step :  376 loss :  0.4178149998188019\n",
      "step :  377 loss :  0.3031143248081207\n",
      "step :  378 loss :  0.13937433063983917\n",
      "step :  379 loss :  0.030642850324511528\n",
      "step :  380 loss :  0.025615230202674866\n",
      "step :  381 loss :  0.0980626568198204\n",
      "step :  382 loss :  0.18147045373916626\n",
      "step :  383 loss :  0.21316584944725037\n",
      "step :  384 loss :  0.1764984279870987\n",
      "step :  385 loss :  0.09586159884929657\n",
      "step :  386 loss :  0.03284939378499985\n",
      "step :  387 loss :  0.0073804548010230064\n",
      "step :  388 loss :  0.01390060968697071\n",
      "step :  389 loss :  0.044755902141332626\n",
      "step :  390 loss :  0.08583316951990128\n",
      "step :  391 loss :  0.12424767762422562\n",
      "step :  392 loss :  0.14506728947162628\n",
      "step :  393 loss :  0.1573486030101776\n",
      "step :  394 loss :  0.14798282086849213\n",
      "step :  395 loss :  0.12568360567092896\n",
      "step :  396 loss :  0.08594480156898499\n",
      "step :  397 loss :  0.044841643422842026\n",
      "step :  398 loss :  0.015017163008451462\n",
      "step :  399 loss :  0.0036836466751992702\n",
      "step :  400 loss :  0.007707036100327969\n",
      "step :  401 loss :  0.02272329106926918\n",
      "step :  402 loss :  0.044645458459854126\n",
      "step :  403 loss :  0.06745299696922302\n",
      "step :  404 loss :  0.09419383853673935\n",
      "step :  405 loss :  0.12390933930873871\n",
      "step :  406 loss :  0.17089802026748657\n",
      "step :  407 loss :  0.22474463284015656\n",
      "step :  408 loss :  0.30613091588020325\n",
      "step :  409 loss :  0.36820632219314575\n",
      "step :  410 loss :  0.4304628372192383\n",
      "step :  411 loss :  0.41906028985977173\n",
      "step :  412 loss :  0.35016539692878723\n",
      "step :  413 loss :  0.21964024007320404\n",
      "step :  414 loss :  0.08643798530101776\n",
      "step :  415 loss :  0.014770453795790672\n",
      "step :  416 loss :  0.023655487224459648\n",
      "step :  417 loss :  0.08623836189508438\n",
      "step :  418 loss :  0.16013339161872864\n",
      "step :  419 loss :  0.2039368748664856\n",
      "step :  420 loss :  0.1946844458580017\n",
      "step :  421 loss :  0.14850881695747375\n",
      "step :  422 loss :  0.09271171689033508\n",
      "step :  423 loss :  0.047196343541145325\n",
      "step :  424 loss :  0.01721755787730217\n",
      "step :  425 loss :  0.00590672018006444\n",
      "step :  426 loss :  0.010722676292061806\n",
      "step :  427 loss :  0.027318771928548813\n",
      "step :  428 loss :  0.050301164388656616\n",
      "step :  429 loss :  0.07688842713832855\n",
      "step :  430 loss :  0.10332508385181427\n",
      "step :  431 loss :  0.13495543599128723\n",
      "step :  432 loss :  0.16075542569160461\n",
      "step :  433 loss :  0.1987438052892685\n",
      "step :  434 loss :  0.20622742176055908\n",
      "step :  435 loss :  0.20384958386421204\n",
      "step :  436 loss :  0.1473258137702942\n",
      "step :  437 loss :  0.09168250858783722\n",
      "step :  438 loss :  0.07787535339593887\n",
      "step :  439 loss :  0.11315849423408508\n",
      "step :  440 loss :  0.15974301099777222\n",
      "step :  441 loss :  0.1722945272922516\n",
      "step :  442 loss :  0.17391657829284668\n",
      "step :  443 loss :  0.19644153118133545\n",
      "step :  444 loss :  0.2563665509223938\n",
      "step :  445 loss :  0.314069002866745\n",
      "step :  446 loss :  0.34381386637687683\n",
      "step :  447 loss :  0.3360031247138977\n",
      "step :  448 loss :  0.30023452639579773\n",
      "step :  449 loss :  0.2523513436317444\n",
      "step :  450 loss :  0.19055765867233276\n",
      "step :  451 loss :  0.12731388211250305\n",
      "step :  452 loss :  0.07140275835990906\n",
      "step :  453 loss :  0.029046546667814255\n",
      "step :  454 loss :  0.010550292208790779\n",
      "step :  455 loss :  0.016151070594787598\n",
      "step :  456 loss :  0.039691925048828125\n",
      "step :  457 loss :  0.07443280518054962\n",
      "step :  458 loss :  0.1144455149769783\n",
      "step :  459 loss :  0.15611213445663452\n",
      "step :  460 loss :  0.19526900351047516\n",
      "step :  461 loss :  0.2293313443660736\n",
      "step :  462 loss :  0.25040310621261597\n",
      "step :  463 loss :  0.2575404644012451\n",
      "step :  464 loss :  0.2477814108133316\n",
      "step :  465 loss :  0.22448419034481049\n",
      "step :  466 loss :  0.19044145941734314\n",
      "step :  467 loss :  0.15061253309249878\n",
      "step :  468 loss :  0.11339262127876282\n",
      "step :  469 loss :  0.08413013070821762\n",
      "step :  470 loss :  0.0663604885339737\n",
      "step :  471 loss :  0.05816026031970978\n",
      "step :  472 loss :  0.05594446882605553\n",
      "step :  473 loss :  0.05593784153461456\n",
      "step :  474 loss :  0.05667433142662048\n",
      "step :  475 loss :  0.05831271782517433\n",
      "step :  476 loss :  0.06367966532707214\n",
      "step :  477 loss :  0.07530580461025238\n",
      "step :  478 loss :  0.09548399597406387\n",
      "step :  479 loss :  0.12521299719810486\n",
      "step :  480 loss :  0.16588415205478668\n",
      "step :  481 loss :  0.21679657697677612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  482 loss :  0.28119802474975586\n",
      "step :  483 loss :  0.35296791791915894\n",
      "step :  484 loss :  0.43652892112731934\n",
      "step :  485 loss :  0.5086459517478943\n",
      "step :  486 loss :  0.5784029960632324\n",
      "step :  487 loss :  0.5929780006408691\n",
      "step :  488 loss :  0.5842443704605103\n",
      "step :  489 loss :  0.49188223481178284\n",
      "step :  490 loss :  0.38907885551452637\n",
      "step :  491 loss :  0.2549743354320526\n",
      "step :  492 loss :  0.14870238304138184\n",
      "step :  493 loss :  0.06784426420927048\n",
      "step :  494 loss :  0.02274903655052185\n",
      "step :  495 loss :  0.007861128076910973\n",
      "step :  496 loss :  0.01680678129196167\n",
      "step :  497 loss :  0.04274555668234825\n",
      "step :  498 loss :  0.08109237253665924\n",
      "step :  499 loss :  0.13029395043849945\n"
     ]
    }
   ],
   "source": [
    "mv_net_x.train()\n",
    "mv_net_y.train()\n",
    "\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]    \n",
    "        \n",
    "        x_batch = torch.tensor(inpt[:,:,0],dtype=torch.float32)\n",
    "        y_batch = torch.tensor(inpt[:,:,1],dtype=torch.float32)\n",
    "        target_batch = torch.tensor(target,dtype=torch.float32)\n",
    "    \n",
    "        mv_net_x.init_hidden(x_batch.size(0))\n",
    "        mv_net_y.init_hidden(y_batch.size(0))\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        x_batch = x_batch.reshape(8,3,1) # 3 채널로 맞춰줘야 함\n",
    "        y_batch = y_batch.reshape(8,3,1)\n",
    "        output_x = mv_net_x(x_batch)\n",
    "        output_y = mv_net_y(y_batch)\n",
    "        loss = criterion(output_x.view(-1), target_batch[0,:]) + criterion(output_y.view(-1), target_batch[1,:])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_x.step()        \n",
    "        optimizer_x.zero_grad()\n",
    "        optimizer_y.step()\n",
    "        optimizer_y.zero_grad()\n",
    "    print('step : ' , t , 'loss : ' , loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
