{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(1208499):\n",
    "    if i % 30 == 0:\n",
    "        diff = df[2].iloc[i + 1] - df[2].iloc[i]\n",
    "        a.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = a[0]\n",
    "for i in range(1,len(a)):\n",
    "    result += a[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result / len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 분리하기\n",
    "1. 데이터가 너무 많다는 점\n",
    "2. 어느 곳에 방문할지에 대한 정보가 좀 더 중요하다는 점\n",
    "3. 사람이 걸을 수 있는 정도가 한정되어 있다는 점 <br>\n",
    "세가지를 고려할 때 모두를 학습시키기 보다는 일부분을 쪼개서 학습시키는 것이 합리적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_30 = []\n",
    "for i in range(0,len(df),30):\n",
    "    df_diff_30.append(df.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_30_pd = pd.DataFrame(df_diff_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df_128_30[0], df_128_30[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    " \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)-n_steps):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-2], sequences[end_ix, -2:]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# x, y\n",
    "# define input sequence\n",
    "in_seq1 = x.values\n",
    "in_seq2 = y.values\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, in_seq1, in_seq2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = split_sequences(dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm 모델 학습\n",
    "    # 어려웠던 점\n",
    "    1. 인풋 사이즈를 맞춰줘야 하는데 코드를 온전히 이해하여야 했기에 많은 어려움이 존재\n",
    "    2. 시계열 데이터이기 때문에 마지막 배치의 경우에 문제가 존재했음 -> 이 부분을 처리하는 것이 중요!\n",
    "    3. 텐서 자체에서 문제가 존재했고, 데이터를 텐서로 변환하는 과정에서 소수점의 사라짐이 존재하게 되었음 이러한 문제점을 해결하는 방법은? torch.set_printoptions(precision=10)으로 소수점을 사라지지 않게 하는 방법\n",
    "    4. 1회 학습시 gpu를 사용해야 했는데, 모델, 파라미터, loss function에 모두 cuda를 할당해줘야 하지만 처음에는 파라미터에만 할당해줘서 어려움이 있었음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda0 = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MV_LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features,seq_length):\n",
    "        super(MV_LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = 100 # number of hidden states\n",
    "        self.n_layers = 300 # number of LSTM layers (stacked)\n",
    "    \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers, \n",
    "                                batch_first = True,dropout=0.1).to(device=cuda0)\n",
    "        # according to pytorch docs LSTM output is \n",
    "        # (batch_size,seq_len, num_directions * hidden_size)\n",
    "        # when considering batch_first = True\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, 1).to(device=cuda0)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        hidden_state = torch.zeros(self.n_layers,batch_size, self.n_hidden, device = cuda0)\n",
    "        cell_state = torch.zeros(self.n_layers,batch_size, self.n_hidden, device = cuda0)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        lstm_out, self.hidden = self.l_lstm(x, self.hidden)\n",
    "        # lstm_out(with batch_first = True) is \n",
    "        # (batch_size,seq_len,num_directions * hidden_size)\n",
    "        # for following linear layer we want to keep batch_size dimension and merge rest       \n",
    "        # .contiguous() -> solves tensor compatibility error\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        return self.l_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성 방법\n",
    "    데이터들을 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1 # this is number of parallel inputs\n",
    "n_timesteps = 5 # this is number of timesteps\n",
    "\n",
    "# convert dataset into input/output\n",
    "X, y = split_sequences(dataset, n_timesteps)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# create NN\n",
    "mv_net_x = MV_LSTM(n_features, n_timesteps)\n",
    "mv_net_y = MV_LSTM(n_features, n_timesteps)\n",
    "criterion = torch.nn.MSELoss().to(device=cuda0)\n",
    " # reduction='sum' created huge loss value\n",
    "optimizer_x = torch.optim.Adam(mv_net_x.parameters(), lr=1e-1)\n",
    "optimizer_y = torch.optim.Adam(mv_net_y.parameters(), lr=1e-1)\n",
    "\n",
    "train_episodes = 30\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_net_x.train()\n",
    "mv_net_y.train()\n",
    "\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]\n",
    "\n",
    "        # x, y 트레인셋\n",
    "        x_batch = torch.tensor(inpt[:,:,0],dtype=torch.float32, device = cuda0) # 텐서로 바꾸면서\n",
    "        y_batch = torch.tensor(inpt[:,:,1],dtype=torch.float32, device = cuda0)\n",
    "        \n",
    "        if x_batch.shape == (batch_size,n_timesteps):\n",
    "            x_batch = x_batch.reshape(batch_size,n_timesteps,1) # 3 채널로 맞춰줘야 함\n",
    "            y_batch = y_batch.reshape(batch_size,n_timesteps,1)\n",
    "\n",
    "            mv_net_x.init_hidden(x_batch.size(0))\n",
    "            mv_net_y.init_hidden(y_batch.size(0))\n",
    "\n",
    "            # x,y의 예측 값\n",
    "            output_x = mv_net_x(x_batch)\n",
    "            output_y = mv_net_y(y_batch)\n",
    "\n",
    "            # x,y의 정답\n",
    "            target_x_batch = torch.tensor(target[:, 0],dtype=torch.float32, device = cuda0)\n",
    "            target_y_batch = torch.tensor(target[:, 1],dtype=torch.float32, device = cuda0)\n",
    "\n",
    "\n",
    "        #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "        #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "\n",
    "            # loss function이 두개의 합이 최소가 되는 쪽으로 loss를 구함\n",
    "            loss = criterion(output_x.view(-1), target_x_batch) + criterion(output_y.view(-1), target_y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer_x.step()        \n",
    "            optimizer_x.zero_grad() # x가 0으로 흐르도록 함\n",
    "\n",
    "            optimizer_y.step()\n",
    "            optimizer_y.zero_grad() # y가 0으로 흐르도록 함\n",
    "    print('step : ' , t , 'loss : ' , loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
